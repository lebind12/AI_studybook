{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13K1EqggxKTxouO1kVAcksQQa95NIC2VO","authorship_tag":"ABX9TyP0dGg3EMDdIHA+UGlXlsG5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"zP66w5gP0U6K","executionInfo":{"status":"ok","timestamp":1685855614908,"user_tz":-540,"elapsed":12221,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","import matplotlib.pyplot as plt\n","\n","# 추가 임포트\n","import pickle\n","from PIL import Image\n","import os\n","import nltk\n","from pycocotools.coco import COCO\n","import torch.utils.data as data\n","import torchvision.models as model\n","import torchvision.transforms as transforms\n","from torch.nn.utils.rnn import pack_padded_sequence # 다양한 길이의 문장에 패딩을 적용해서 고정길이로 변환"]},{"cell_type":"code","source":["from collections import Counter"],"metadata":{"id":"uUZQVetUC1ll","executionInfo":{"status":"ok","timestamp":1685855614908,"user_tz":-540,"elapsed":5,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTF62UI2CSlE","executionInfo":{"status":"ok","timestamp":1685855616109,"user_tz":-540,"elapsed":1205,"user":{"displayName":"이우진","userId":"02891789518284348743"}},"outputId":"4b50ed30-9dc3-4e92-8e33-98af3b969463"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# 이미지 캡션 데이터셋 다운로드\n","\n","1. Training은 13GB, Validation은 6GB\n","2. 밤새 코드를 실행해놓으라고?\n","3. https://cocodataset.org/#home 가 출처 "],"metadata":{"id":"0WSe6e580vL1"}},{"cell_type":"code","source":["!ls \"./drive/MyDrive/Colab Notebooks/AI_study\"\n","# !mkdir \"dataset\"\n","!cd \"dataset\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jpUtk6Ri2g86","executionInfo":{"status":"ok","timestamp":1685855616462,"user_tz":-540,"elapsed":355,"user":{"displayName":"이우진","userId":"02891789518284348743"}},"outputId":"19fda387-0282-4459-8a9b-35b3564f86ec"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2장_파이토치_맛보기.ipynb\t mnist_pytorch_practice.ipynb\n","for_git_commit.ipynb\t\t 파이토치교과서_2장_연습.ipynb\n","image_captioning_practice.ipynb\n","/bin/bash: line 0: cd: dataset: No such file or directory\n"]}]},{"cell_type":"code","source":["import urllib\n","import zipfile\n","from tqdm import tqdm\n","\n","#https://stackoverflow.com/a/53877507/1558946\n","class DownloadProgressBar(tqdm):\n","    def update_to(self, b=1, bsize=1, tsize=None):\n","        if tsize is not None:\n","            self.total = tsize\n","        self.update(b * bsize - self.n)\n","\n","def download_data(url):\n","    print(f\"{url} 다운로드 중 ...\")\n","    with DownloadProgressBar(unit='B', unit_scale=True,\n","                             miniters=1, desc=url.split('/')[-1]) as t:\n","        zip_path, _ = urllib.request.urlretrieve(url, reporthook=t.update_to)\n","\n","    print(\"압축을 푸는 중 ...\")\n","    with zipfile.ZipFile(zip_path, \"r\") as f:\n","        for name in tqdm(iterable=f.namelist(), total=len(f.namelist())):\n","            f.extract(member=name, path=\"data_dir\")\n","\n","\n","# download_data(\"http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\")\n","download_data(\"http://images.cocodataset.org/zips/train2014.zip\")\n","download_data(\"http://images.cocodataset.org/zips/val2014.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLwMUbJ81oQZ","executionInfo":{"status":"ok","timestamp":1685856217196,"user_tz":-540,"elapsed":600736,"user":{"displayName":"이우진","userId":"02891789518284348743"}},"outputId":"08cd3dd6-4a1a-4cfb-b210-99722748ba75"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["http://images.cocodataset.org/zips/train2014.zip 다운로드 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["train2014.zip: 13.5GB [03:34, 62.9MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["압축을 푸는 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 82784/82784 [03:31<00:00, 391.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["http://images.cocodataset.org/zips/val2014.zip 다운로드 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["val2014.zip: 6.65GB [01:45, 62.8MB/s]                            \n"]},{"output_type":"stream","name":"stdout","text":["압축을 푸는 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 40505/40505 [01:05<00:00, 619.46it/s]\n"]}]},{"cell_type":"code","source":["download_data(\"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNmDPVzV152M","executionInfo":{"status":"ok","timestamp":1685856229023,"user_tz":-540,"elapsed":11830,"user":{"displayName":"이우진","userId":"02891789518284348743"}},"outputId":"ff80d07f-906d-4066-c955-9412be2de601"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["http://images.cocodataset.org/annotations/annotations_trainval2014.zip 다운로드 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["annotations_trainval2014.zip: 253MB [00:02, 88.1MB/s]                          \n"]},{"output_type":"stream","name":"stdout","text":["압축을 푸는 중 ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6/6 [00:08<00:00,  1.49s/it]\n"]}]},{"cell_type":"code","source":["class Vocab(object):\n","    def __init__(self):\n","        self.w2i = {} # word to index\n","        self.i2w = {} # index to word\n","        self.index = 0 # adding value\n","    def __call__(self, token):\n","        if not token in self.w2i:\n","            return self.w2i['<unk>']\n","        return self.w2i[token]\n","    def __len__(self):\n","        return len(self.w2i)\n","    def add_token(self, token):\n","        if not token in self.w2i:\n","            self.w2i[token] = self.index\n","            self.i2w[self.index] = token\n","            self.index += 1"],"metadata":{"id":"fRo0Ti78Dj0K","executionInfo":{"status":"ok","timestamp":1685856229283,"user_tz":-540,"elapsed":278,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["실제 텍스트 토큰을 숫자 토큰으로 전환할 수 있는 사전 구축."],"metadata":{"id":"o1H_tg1O7Gyj"}},{"cell_type":"code","source":["def build_vocabulary(json, threshold):\n","    # 사전 wrapper 구축\n","    coco = COCO(json)\n","    counter = Counter()\n","    ids = coco.anns.keys()\n","    for i, id in enumerate(ids):\n","        caption = str(coco.anns[id]['caption'])\n","        tokens = nltk.tokenize.word_tokenize(caption.lower())\n","        counter.update(tokens)\n","        if (i+1) % 1000 == 0:\n","            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n","    tokens = [token for token, cnt in counter.items()]\n","    vocab = Vocab()\n","    vocab.add_token('<pad>')\n","    vocab.add_token('<start>')\n","    vocab.add_token('<end>')\n","    vocab.add_token('<unk>')\n","\n","    for i, token in enumerate(tokens):\n","        vocab.add_token(token)\n","    return vocab"],"metadata":{"id":"2H7MQR7TCnoW","executionInfo":{"status":"ok","timestamp":1685856229283,"user_tz":-540,"elapsed":3,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["vocab = build_vocabulary(json = 'data_dir/annotations/captions_train2014.json',\n","                         threshold = 4)\n","vocab_path = './data_dir/vocabulary.pkl'\n","with open(vocab_path, 'wb') as f:\n","    pickle.dump(vocab, f)\n","print(\"Total vocabulary size: {}\".format(len(vocab)))\n","print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"],"metadata":{"id":"Vi-s1-nbDN4i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir data_dir/resized_images"],"metadata":{"id":"ugBve1kvHXB5","executionInfo":{"status":"ok","timestamp":1685860512051,"user_tz":-540,"elapsed":292,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def reshape_image(image, shape):\n","    return image.resize(shape, Image.ANTIALIAS)\n","def reshape_images(image_path, output_path, shape):\n","    images = os.listdir(image_path)\n","    num_im = len(images)\n","    for i, im in enumerate(images):\n","        with open(os.path.join(image_path, im), 'r+b') as f:\n","            with Image.open(f) as image:\n","                image = reshape_image(image, shape)\n","                image.save(os.path.join(output_path, im), image.format)\n","            \n","        if (i+1) % 100 == 0:\n","            print(\"[{}/{}] Resized the images and saved into '{}'.\".format(i+1, num_im, output_path))\n","image_path = './data_dir/train2014/'\n","output_path = './data_dir/resized_images/'\n","image_shape = [256, 256]\n","reshape_images(image_path, output_path, image_shape)\n"],"metadata":{"id":"fE7rlLVbFne8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 깃헙 업데이트 테스트"],"metadata":{"id":"3jdnpLG8H1ds","executionInfo":{"status":"ok","timestamp":1685914085359,"user_tz":-540,"elapsed":320,"user":{"displayName":"이우진","userId":"02891789518284348743"}}},"execution_count":13,"outputs":[]}]}